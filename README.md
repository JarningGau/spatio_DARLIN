# Spatio_DARLIN

This is a Snakemake pipeline for automated preprocessing of spatial lineage tracing data from SpatioDARLIN. This repository is a fork of [snakemake_DARLIN](https://github.com/ShouWenWang-Lab/snakemake_DARLIN), modified to support spatial lineage tracing data generated by DARLIN mouse and the [BMKMANU S3000](http://www.biomarker.com.cn/zhizao/s3000) platform. 

The preprocessing pipeline includes:
- Lineage barcode identification and quality control
- Spatial barcode parsing
- Allele annotation
- Grouping spots into segmented cells
- Generating final clone-by-spots and clone-by-segmented-cells matrices

## Requirements

- **Conda** (for environment management)
- **MATLAB** (must be available in command line interface)
- **Python 3.9**
- **Snakemake 7.24.0**
- **BSTMatrix** (quantification pipeline for BMKMANU S3000)

## Installation

### 1. Install BSTMatrix

```bash
cd /path/to/tools
wget http://www.bmkmanu.com/wp-content/uploads/2024/07/BSTMatrix_v2.4.f.1.zip
unzip BSTMatrix_v2.4.f.1.zip
## conda env for BSTMatrix
cd BSTMatrix_v2.4.f.1
conda env create -n BST-env -f environment.yaml

export PATH=/path/to/tools/BSTMatrix_v2.4.f.1:$PATH
```

### 2. Create a conda environment

```bash
kernel_name='spatio_darlin'
conda create -n $kernel_name python=3.9 --yes
conda activate $kernel_name
conda install -c conda-forge -c bioconda snakemake=7.24.0 --yes
pip install jupyterlab umi_tools seaborn papermill biopython cutadapt
pip install numpy==1.24.4
python -m ipykernel install --user --name=$kernel_name
```

### 3. Install required Python packages and MATLAB code

Install the following dependencies in your desired code directory:

```bash
code_directory='.' # change it to the directory where you want to put the packages
cd $code_directory

# Install spatio_DARLIN (this repository)
# If you haven't already cloned this repo, run:
# git clone https://github.com/JarningGau/spatio_DARLIN --depth=1
cd spatio_DARLIN  # or navigate to where you cloned this repository
python setup.py develop
cd ..

# Install MosaicLineage
pip install cospar toolz  # dependencies for MosaicLineage
git clone https://github.com/ShouWenWang-Lab/MosaicLineage --depth=1
cd MosaicLineage
python setup.py develop
cd ..

# Download MATLAB code Custom_CARLIN for allele annotation
mkdir -p CARLIN_pipeline
cd CARLIN_pipeline
git clone https://github.com/ShouWenWang-Lab/Custom_CARLIN --depth=1
cd ..
```

**Note:** Ensure MATLAB is installed and available in your command line interface (accessible via `matlab` command).

## Usage

### Quick Start (Run Test)

To test the pipeline with example data:

```bash
conda activate $kernel_name
cd test
bash download_bmk.sh
```

This will download the test data. After downloading, you can run the test pipeline:

```bash
bash test_bmk.sh
```

### Input Data Structure

The pipeline expects the following input data structure:

```
data/BMKS3000/
├── fastq/                    # Sequencing reads
│   ├── <sample>_<locus>_R1.fastq.gz
│   └── <sample>_<locus>_R2.fastq.gz
├── images/                   # Image files for BSTMatrix pipeline
│   ├── <sample>_FL.tif
│   ├── <sample>_HE.tif
│   └── <sample>_HE.txt
└── segmentation/            # Cell segmentation results from BSTMatrix
    └── <sample>/
        ├── all_barcode_num.txt      # Spots -> cellbin relationship
        └── barcodes_pos.tsv.gz      # Spatial barcode positions
```

**Input file descriptions:**
- **FASTQ files**: Paired-end sequencing reads. Naming convention: `<sample>_<locus>_R1.fastq.gz` and `<sample>_<locus>_R2.fastq.gz`, where `<locus>` can be `CA`, `RA`, or `TA`.
- **Image files**: Required for BSTMatrix pipeline
  - `<sample>_FL.tif`: Fluorescence image
  - `<sample>_HE.tif`: H&E stained image
  - `<sample>_HE.txt`: Image metadata
- **Segmentation files**: Generated from BSTMatrix on mRNA data
  - `all_barcode_num.txt`: Maps spots to cell bins
  - `barcodes_pos.tsv.gz`: Spatial coordinates of barcodes

### Configuration Files

Each analysis requires a YAML configuration file. The test directory contains example configs:

```
test_BMKS3000/
├── config-CA.yaml    # Configuration for CA locus
├── config-RA.yaml    # Configuration for RA locus
└── config-TA.yaml    # Configuration for TA locus
```

### Configuration File Example

Below is an example configuration file with explanations:

```yaml
# Sample list to process
SampleList: ['L0927_Brain']

# Template type: 'Tigre_2022_v2' (TA), 'Rosa_v2' (RA), or 'cCARLIN' (CA)
template: 'cCARLIN'

# Directory paths (relative to the config file location)
raw_fastq_dir: '../data/BMKS3000/fastq'
image_dir: '../data/BMKS3000/images'
segmentation_dir: '../data/BMKS3000/segmentation'

# Cutadapt parameters for quality filtering
cutadapt:
    base_quality_cutoff: 10
    cores: 8

# Python DARLIN pipeline parameters
python_DARLIN_pipeline:
    # [Sequences level] Drop sequences with fewer supported reads (sequencing error filtering)
    # This is an important parameter for quality control
    reads_cutoff_denoise: 4
    
    # [Lineage barcode level] Denoise lineage barcode (amplification error correction)
    # Relative threshold: 0.05 for 5% error rate, will be multiplied by sequence length
    # Set to None to disable relative threshold
    distance_relative_threshold: None
    distance_absolute_threshold: 1
    
    # [Spots level] Drop spots with read_counts < slope_threshold * umi_counts (suspicious spots)
    slope_threshold: 2
    
    # [Spots level] Drop spots without dominant clone (lineage barcode / mRNA diffusion filtering)
    read_fraction_per_clone_spot_cutoff: 0.2
    
    # [Spots level] Drop spots with fewer supported reads within each lineage barcode group
    # The reads_cutoff is determined by: 
    # np.max([min_reads_per_allele_group, perc_reads_per_allele_group * df['read'].max()])
    min_reads_per_allele_group: 4  # Only works when >= reads_cutoff_denoise
    perc_reads_per_allele_group: 0.01
    
    # Jupyter kernel name
    kernel: 'spatio_darlin'
```

### Output Files

After a successful run (using the bundled test configs or your own), the workspace will resemble:

```text
test_BMKS3000/
├── BST_config/      # BSTMatrix configuration files
├── BST_output/      # Outputs from BSTMatrix
├── config-*.yaml    # Input configs (CA/RA/TA)
├── cutadapt/        # Primer-trimmed FASTQs: reads1, spatial barcode + UMI; reads2, lineage barcode
├── DARLIN/          # Intermediate DARLIN pipeline products
├── outs/            # Aggregated results
└── slim_fastq/      # FASTQs for allele annotation
```

The final results live in `test_BMKS3000/outs/`:

```text
test_BMKS3000/outs/
└── L0927_Brain_CA/
    ├── all.done
    ├── cellbin/        # Cell-bin level matrices
    ├── level_1         # spots-bin, level 1 matrices (3μm)
    ├── ...
    └── level_18        # spots-bin, level 18 matrices (99μm)
```

| Level           | 18   | 9    | 7    | 6    | 5    | 4    | 3    | 2    | 1    |
| --------------- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- | ---- |
| Resolution (μm) | 99   | 48   | 37   | 31   | 25   | 20   | 14   | 8    | 3    |

### Running the Pipeline

To run the pipeline with your own data:

1. Create a configuration file following the example above
2. Ensure your input data follows the expected structure
3. Run Snakemake:

```bash
conda activate $kernel_name
snakemake --snakefile snakefiles/snakefile_DARLIN_BMKS3000.py --configfile <your_config.yaml> -c <cores>
```

Replace `<your_config.yaml>` with the path to your configuration file and `<cores>` with the number of CPU cores to use.



## Additional Resources

For upstream analysis of BMKMANU S3000 spatial transcriptomics data, see the [upstream analysis documentation](doc/BMKS3000_upstream.md).
